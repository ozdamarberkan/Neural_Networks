{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A\n",
    "\n",
    "f = h5py.File('assign3_data3.h5', 'r')\n",
    "dataKeys = list(f.keys())\n",
    "print('The data keys are:' + str(dataKeys))\n",
    "\n",
    "# Gathering the  train images, test images, train labels and test labels.\n",
    "train_data = f['trX']\n",
    "train_labels = f['trY']\n",
    "test_data = f['tstX']\n",
    "test_labels = f['tstY']\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print('The size of train data is: ' + str(np.shape(train_data)))\n",
    "print('The size of train labels is: ' + str(np.shape(train_labels)))\n",
    "print('The size of test_data is: ' + str(np.shape(test_data)))\n",
    "print('The size of test_labels is: ' + str(np.shape(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(fan_in, fan_out, wb_shape):\n",
    "                   \n",
    "    np.random.seed(8)   \n",
    "    \n",
    "    lim = np.sqrt(6/(fan_in+fan_out))  \n",
    "\n",
    "    weight = np.random.uniform(-lim, lim, size=(wb_shape))\n",
    "\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, input_dim = 3, hidden_dim = 128, seq_len = 150, learning_rate = 1e-1,\n",
    "                 momentumCoef = 0.85, output_class = 6, momentum_condition = False):\n",
    "\n",
    "        np.random.seed(8)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.output_class = output_class\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.momentumCoef = momentumCoef\n",
    "        self.momentum_condition = momentum_condition\n",
    "        self.last_t = 149\n",
    "        \n",
    "        # Weight initialization\n",
    "\n",
    "        self.W1 = initialize_weights(self.input_dim, self.hidden_dim,  (self.input_dim,self.hidden_dim))\n",
    "        self.B1 = initialize_weights(self.input_dim, self.hidden_dim,  (1,self.hidden_dim))\n",
    "        \n",
    "        self.W1_rec = initialize_weights(self.hidden_dim, self.hidden_dim,  (self.hidden_dim,self.hidden_dim))\n",
    "        \n",
    "        self.W2 = initialize_weights(self.hidden_dim, self.output_class,  (self.hidden_dim,self.output_class))        \n",
    "        self.B2 = initialize_weights(self.hidden_dim, self.output_class,  (1,self.output_class))\n",
    "\n",
    "        # momentum updates\n",
    "        \n",
    "        self.momentum_W1 = 0\n",
    "        self.momentum_B1 = 0\n",
    "        self.momentum_W1_rec = 0 \n",
    "        self.momentum_W2 = 0 \n",
    "        self.momentum_B2 = 0\n",
    "        \n",
    "    def accuracy(self, y, y_pred):\n",
    "        '''\n",
    "        MCE is the accuracy of our network. Mean classification error will be calculated to find accuracy.\n",
    "        INPUTS:\n",
    "\n",
    "            y            : y is the labels for our data.\n",
    "            y_pred       : y_pred is the network's prediction.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "                         : returns the accuracy between y and y_pred.\n",
    "        '''         \n",
    "        count = 0\n",
    "        for i in range(len(y)):\n",
    "            if(y[i] == y_pred[i]):\n",
    "                count += 1\n",
    "        N = np.shape(y)[0]\n",
    "\n",
    "        return 100 * (count / N)        \n",
    "\n",
    "    def tanh(self, x):\n",
    "        '''\n",
    "        This function is the hyperbolic tangent for the activation functions of each neuron.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the weighted sum which will be pushed to activation function.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the hyperbolic tangent of the input x.\n",
    "        ''' \n",
    "        \n",
    "        result = 2 / (1 + np.exp(-2*x)) - 1\n",
    "        return result    \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \n",
    "        '''\n",
    "        This function is the sigmoid for the activation function.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the weighted sum which will be pushed to activation function.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the sigmoid of the input x.\n",
    "        '''        \n",
    "        \n",
    "        result = 1 / (1 + np.exp(-x))\n",
    "        return result\n",
    "    \n",
    "    def der_sigmoid(self, x):\n",
    "        '''\n",
    "        This function is the derivative of sigmoid function.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the input.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the derivative of sigmoid of the input x.\n",
    "        '''         \n",
    "        \n",
    "        \n",
    "        result = self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "        return result\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \n",
    "        '''\n",
    "        This function is the softmax for the activation function of output layer.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the weighted sum which will be pushed to activation function.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the softmax of the input x.\n",
    "        '''         \n",
    "        \n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        result = e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        return result\n",
    "    \n",
    "    def der_softmax(self, x):\n",
    "        \n",
    "        '''\n",
    "        This function is the derivative of softmax.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the input.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the derivative of softmax of the input x.\n",
    "        '''           \n",
    "        \n",
    "        p = self.softmax(x)\n",
    "        result = p * (1-p)\n",
    "        return result\n",
    "    \n",
    "    def CategoricalCrossEntropy(self, y, y_pred):\n",
    "        \n",
    "        '''\n",
    "        cross_entropy is the loss function for the network. \n",
    "        INPUTS:\n",
    "\n",
    "            y            : y is the labels for our data.\n",
    "            y_pred       : y_pred is the network's prediction.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            cost         : cost is the cross entropy error between y and y_pred.\n",
    "        '''         \n",
    "        \n",
    "        # To avoid 0\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "        cost = -np.mean(y * np.log(y_pred + 1e-15))\n",
    "        return cost\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "\n",
    "        data_state = dict()\n",
    "        hidden_state = dict()\n",
    "        output_state = dict()\n",
    "        probabilities = dict()\n",
    "        \n",
    "        self.h_prev_state = np.zeros((1,self.hidden_dim))\n",
    "        hidden_state[-1] = self.h_prev_state\n",
    "        # Loop over time T = 150 :\n",
    "        \n",
    "        for t in range(self.seq_len):\n",
    "\n",
    "            data_state[t] = data[:,t]\n",
    "            # Recurrent hidden layer computations:\n",
    "\n",
    "            hidden_state[t] = self.tanh(np.dot(data_state[t], self.W1) + np.dot(hidden_state[t-1], self.W1_rec) + self.B1)\n",
    "            output_state[t] = np.dot(hidden_state[t], self.W2) + self.B2\n",
    "            # The probabilities per class\n",
    "\n",
    "            probabilities[t] = self.softmax(output_state[t])\n",
    "        \n",
    "        cache = [data_state, hidden_state, probabilities]\n",
    "        return cache\n",
    "    \n",
    "    def BPTT(self,data,Y):\n",
    "        \n",
    "        cache = self.forward(data)\n",
    "        \n",
    "        data_state, hidden_state, probs = cache\n",
    "\n",
    "        dW1, dW1_rec, dW2 = np.zeros((np.shape(self.W1))), np.zeros((np.shape(self.W1_rec))), np.zeros((np.shape(self.W2)))\n",
    "        dB1, dB2 = np.zeros((np.shape(self.B1))), np.zeros((np.shape(self.B2)))\n",
    "        dhnext = np.zeros((np.shape(hidden_state[0])))\n",
    "        \n",
    "        dy = probs[self.last_t]\n",
    "        dy[np.arange(len(Y)),np.argmax(Y,1)] -= 1\n",
    "        dB2 += np.sum(dy,axis = 0, keepdims = True)\n",
    "        \n",
    "        \n",
    "        dW2 += np.dot(hidden_state[self.last_t].T,dy)\n",
    "        \n",
    "        for t in reversed(range(1,self.seq_len)):\n",
    "            dh = np.dot(dy, self.W2.T) + dhnext\n",
    "            dh_rec = (1 - (hidden_state[t] * hidden_state[t])) * dh\n",
    "            dB1 += np.sum(dh_rec,axis = 0, keepdims = True)\n",
    "            dW1 += np.dot(data_state[t].T, dh_rec)\n",
    "            dW1_rec += np.dot(hidden_state[t-1].T, dh_rec)\n",
    "            dhnext = np.dot(dh_rec, self.W1_rec.T)\n",
    "\n",
    "        grads = [dW1,dB1,dW1_rec,dW2,dB2]\n",
    "        \n",
    "        \n",
    "        for grad in grads:\n",
    "            np.clip(grad, -10, 10, out = grad)\n",
    "            \n",
    "        return grads, cache\n",
    "    \n",
    "        \n",
    "    def update_weights(self,data,Y):\n",
    "        \n",
    "        grads, cache = self.BPTT(data,Y)\n",
    "        dW1,dB1,dW1_rec,dW2,dB2 = grads\n",
    "        sample_size = np.shape(cache)[0]\n",
    "        # If momentum is used.\n",
    "        if( self.momentum_condition == True ):\n",
    "\n",
    "            self.momentum_W1 = dW1 + (self.momentumCoef * self.momentum_W1) \n",
    "            self.momentum_B1 = dB1 + (self.momentumCoef * self.momentum_B1) \n",
    "            self.momentum_W1_rec = dW1_rec + (self.momentumCoef * self.momentum_W1_rec) \n",
    "            self.momentum_W2 = dW2 + (self.momentumCoef * self.momentum_W2) \n",
    "            self.momentum_B2 = dB2 + (self.momentumCoef * self.momentum_B2) \n",
    "                \n",
    "            self.W1 -= self.learning_rate * self.momentum_W1  /sample_size\n",
    "            self.B1 -= self.learning_rate * self.momentum_B1 /sample_size\n",
    "            self.W1_rec -= self.learning_rate * self.momentum_W1_rec /sample_size\n",
    "            self.W2 -= self.learning_rate * self.momentum_W2 /sample_size\n",
    "            self.B2 -= self.learning_rate * self.momentum_B2 /sample_size\n",
    "            \n",
    "        # If momentum is not used.        \n",
    "        else:\n",
    "            \n",
    "            self.W1 -= self.learning_rate * dW1 /sample_size\n",
    "            self.B1 -= self.learning_rate * dB1 / sample_size\n",
    "            self.W1_rec -= self.learning_rate * dW1_rec / sample_size\n",
    "            self.W2 -= self.learning_rate * dW2 / sample_size\n",
    "            self.B2 -= self.learning_rate * dB2 / sample_size\n",
    "            \n",
    "        return cache\n",
    "    \n",
    "    def train_network(self, data, labels, test_data, test_labels, epochs = 50, batch_size = 32):\n",
    "\n",
    "        np.random.seed(8)\n",
    "\n",
    "        valid_loss = list()\n",
    "        valid_accuracy = list()\n",
    "\n",
    "        test_loss = list()\n",
    "        test_accuracy = list()\n",
    "\n",
    "        sample_size = np.shape(data)[0]        \n",
    "        k = int(sample_size / 10)    \n",
    "        \n",
    "        for i in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch : ' +str(i))\n",
    "            randomIndexes = np.random.permutation(sample_size)\n",
    "            data = data[randomIndexes]\n",
    "            \n",
    "            number_of_batches = int(sample_size / batch_size)\n",
    "            for j in range(number_of_batches):\n",
    "                \n",
    "                start = int(batch_size*j)\n",
    "                end = int(batch_size*(j+1))                \n",
    "                \n",
    "                data_feed = data[start:end]\n",
    "                labels_feed = labels[start:end]\n",
    "                \n",
    "                cache_train = self.update_weights(data_feed, labels_feed)\n",
    "             \n",
    "            \n",
    "            valid_data = data[0:k]\n",
    "            valid_labels = labels[0:k]\n",
    "            \n",
    "            probs_valid, predictions_valid = self.predict(valid_data)\n",
    "            \n",
    "            cross_loss_valid = self.CategoricalCrossEntropy(valid_labels, probs_valid[self.last_t])            \n",
    "            acc_valid = self.accuracy(np.argmax(valid_labels,1), predictions_valid)\n",
    "            \n",
    "            \n",
    "            probs_test, predictions_test = self.predict(test_data)\n",
    "                     \n",
    "            cross_loss_test = self.CategoricalCrossEntropy(test_labels, probs_test[self.last_t])\n",
    "            acc_test = self.accuracy(np.argmax(test_labels,1) ,predictions_test)\n",
    "\n",
    "            valid_loss.append(cross_loss_valid)\n",
    "            valid_accuracy.append(acc_valid)\n",
    "\n",
    "            test_loss.append(cross_loss_test)\n",
    "            test_accuracy.append(acc_test)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print('Training time for 1 epoch : ' +str(end_time - start_time))\n",
    "        valid_loss = np.array(valid_loss)\n",
    "        valid_accuracy = np.array(valid_accuracy)\n",
    "\n",
    "        test_loss = np.array(test_loss)\n",
    "        test_accuracy = np.array(test_accuracy)\n",
    "        \n",
    "        return valid_loss, valid_accuracy, test_loss, test_accuracy\n",
    "              \n",
    "    def predict(self,X):\n",
    "        \n",
    "        cache = self.forward(X)\n",
    "        probabilities = cache[-1]\n",
    "        result = np.argmax(probabilities[self.last_t],axis=1)\n",
    "        return probabilities, result\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = RNN(input_dim = 3, hidden_dim = 128, learning_rate = 1e-12, momentumCoef = 0.85, \n",
    "                output_class = 6, momentum_condition = True)\n",
    "\n",
    "valid_loss, valid_accuracy, test_loss, test_accuracy = RNN_model.train_network(train_data, train_labels, test_data, \n",
    "                                                                               test_labels, epochs = 27, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figureNum = 0\n",
    "\n",
    "plt.figure(figureNum)\n",
    "plt.plot(valid_loss)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Cross Entropy for Validation Data over Epochs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(labels, y_pred):\n",
    "    labels_ = np.argmax(labels,1)\n",
    "    result = np.zeros((6, 6))\n",
    "    \n",
    "    for i in range(len(labels_)):\n",
    "        lab_i = labels_[i]\n",
    "        y_pred_i = y_pred[i]\n",
    "        result[lab_i,y_pred_i] +=1\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_(confusion_matrix):\n",
    "    accuracy = 0\n",
    "    all_sum = 0\n",
    "    for i in range(np.shape(confusion_matrix)[0]):\n",
    "        for j in range(np.shape(confusion_matrix)[1]):\n",
    "            all_sum += confusion_matrix[i,j]\n",
    "            if (i == j):\n",
    "                accuracy += confusion_matrix[i,j]\n",
    "                \n",
    "    return accuracy / all_sum * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,train_preds = RNN_model.predict(train_data)\n",
    "_,test_preds = RNN_model.predict(test_data)\n",
    "\n",
    "confusion_mat_train = confusion_matrix(train_labels,train_preds)\n",
    "\n",
    "confusion_mat_test = confusion_matrix(test_labels,test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_RNN_train = accuracy_(confusion_mat_train)\n",
    "print('Accuracy of RNN with train data : ' +str(accuracy_RNN_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_RNN_test = accuracy_(confusion_mat_test)\n",
    "print('Accuracy of RNN with test data : ' +str(accuracy_RNN_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns are : PREDICTION \\n')\n",
    "print('Rows are : ACTUAL \\n')\n",
    "print('The confusion matrix for the training data : \\n \\n' +str(confusion_mat_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns are : PREDICTION \\n')\n",
    "print('Rows are : ACTUAL \\n')\n",
    "\n",
    "print('The confusion matrix for the test data : \\n \\n' +str(confusion_mat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "\n",
    "    def __init__(self,input_dim = 3,hidden_dim = 100, output_class = 6, seq_len = 150,\n",
    "                 batch_size = 30, learning_rate = 1e-1, momentumCoef = 0.85, momentum_condition = False):\n",
    "\n",
    "        np.random.seed(150)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Unfold case T = 150 :\n",
    "        self.seq_len = seq_len\n",
    "        self.output_class = output_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.momentumCoef = momentumCoef\n",
    "        self.momentum_condition = momentum_condition\n",
    "        self.input_stack_dim = self.input_dim + self.hidden_dim\n",
    "        self.last_t = 149\n",
    "        # Weight initialization\n",
    "\n",
    "        self.W_f = initialize_weights(self.input_dim, self.hidden_dim,  (self.input_stack_dim,self.hidden_dim))\n",
    "        self.B_f = initialize_weights(self.input_dim, self.hidden_dim,  (1,self.hidden_dim))\n",
    "        self.W_i = initialize_weights(self.input_dim, self.hidden_dim,  (self.input_stack_dim,self.hidden_dim))\n",
    "        self.B_i = initialize_weights(self.input_dim, self.hidden_dim,  (1,self.hidden_dim))\n",
    "        self.W_c = initialize_weights(self.input_dim, self.hidden_dim,  (self.input_stack_dim,self.hidden_dim))\n",
    "        self.B_c = initialize_weights(self.input_dim, self.hidden_dim,  (1,self.hidden_dim))\n",
    "        self.W_o = initialize_weights(self.input_dim, self.hidden_dim,  (self.input_stack_dim,self.hidden_dim))\n",
    "        self.B_o = initialize_weights(self.input_dim, self.hidden_dim,  (1,self.hidden_dim))\n",
    "        \n",
    "        self.W = initialize_weights(self.hidden_dim, self.output_class,  (self.hidden_dim, self.output_class))\n",
    "        self.B = initialize_weights(self.hidden_dim, self.output_class,  (1, self.output_class))        \n",
    "        \n",
    "\n",
    "        # To keep previous updates in momentum :\n",
    "        self.momentum_W_f = 0\n",
    "        self.momentum_B_f = 0\n",
    "        self.momentum_W_i = 0\n",
    "        self.momentum_B_i = 0\n",
    "        self.momentum_W_c = 0\n",
    "        self.momentum_B_c = 0\n",
    "        self.momentum_W_o = 0\n",
    "        self.momentum_B_o = 0\n",
    "        self.momentum_W = 0\n",
    "        self.momentum_B = 0\n",
    "\n",
    "\n",
    "\n",
    "    def accuracy(self, y, y_pred):\n",
    "        '''\n",
    "        MCE is the accuracy of our network. Mean classification error will be calculated to find accuracy.\n",
    "        INPUTS:\n",
    "\n",
    "            y            : y is the labels for our data.\n",
    "            y_pred       : y_pred is the network's prediction.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "                         : returns the accuracy between y and y_pred.\n",
    "        '''         \n",
    "        count = 0\n",
    "        for i in range(len(y)):\n",
    "            if(y[i] == y_pred[i]):\n",
    "                count += 1\n",
    "        N = np.shape(y)[0]\n",
    "\n",
    "        return 100 * (count / N)        \n",
    "\n",
    "    def tanh(self, x):\n",
    "        '''\n",
    "        This function is the hyperbolic tangent for the activation functions of each neuron.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the weighted sum which will be pushed to activation function.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the hyperbolic tangent of the input x.\n",
    "        ''' \n",
    "        \n",
    "        result = 2 / (1 + np.exp(-2*x)) - 1\n",
    "        return result    \n",
    "\n",
    "    def der_tanh(self, x):\n",
    "        '''\n",
    "        This function is the derivative hyperbolic tangent. This function will be used in backpropagation.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the input.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the derivative of hyperbolic tangent of the input x.\n",
    "        ''' \n",
    "        result = 1 - self.tanh(x)**2\n",
    "        return result      \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \n",
    "        '''\n",
    "        This function is the sigmoid for the activation function.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the weighted sum which will be pushed to activation function.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the sigmoid of the input x.\n",
    "        '''        \n",
    "        \n",
    "        result = 1 / (1 + np.exp(-x))\n",
    "        return result\n",
    "    \n",
    "    def der_sigmoid(self, x):\n",
    "        '''\n",
    "        This function is the derivative of sigmoid function.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the input.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the derivative of sigmoid of the input x.\n",
    "        '''         \n",
    "        \n",
    "        \n",
    "        result = self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "        return result\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \n",
    "        '''\n",
    "        This function is the softmax for the activation function of output layer.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the weighted sum which will be pushed to activation function.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the softmax of the input x.\n",
    "        '''         \n",
    "        \n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        result = e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        return result\n",
    "    \n",
    "    def der_softmax(self, x):\n",
    "        \n",
    "        '''\n",
    "        This function is the derivative of softmax.\n",
    "        INPUTS:\n",
    "\n",
    "            x            : x is the input.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            result       : result is the derivative of softmax of the input x.\n",
    "        '''           \n",
    "        \n",
    "        p = self.softmax(x)\n",
    "        result = p * (1-p)\n",
    "        return result\n",
    "    \n",
    "    def CategoricalCrossEntropy(self, y, y_pred):\n",
    "        \n",
    "        '''\n",
    "        cross_entropy is the loss function for the network. \n",
    "        INPUTS:\n",
    "\n",
    "            y            : y is the labels for our data.\n",
    "            y_pred       : y_pred is the network's prediction.\n",
    "\n",
    "        RETURNS:\n",
    "        \n",
    "            cost         : cost is the cross entropy error between y and y_pred.\n",
    "        '''         \n",
    "        \n",
    "        # To avoid 0\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "        cost = -np.mean(y * np.log(y_pred + 1e-15))\n",
    "        return cost\n",
    "                \n",
    "        \n",
    "        \n",
    "    def cell_forward(self,X,h_prev,C_prev):\n",
    "\n",
    "        #print(X.shape,h_prev.shape)\n",
    "        # Stacking previous hidden state vector with inputs:\n",
    "        stack = np.column_stack([X, h_prev])\n",
    "\n",
    "        # Forget gate:\n",
    "        forget_gate = self.sigmoid(np.dot(stack,self.W_f) + self.B_f)\n",
    "\n",
    "        # İnput gate:\n",
    "        input_gate = self.sigmoid(np.dot(stack,self.W_i) + self.B_i)\n",
    "\n",
    "        # New candidate:\n",
    "        cell_bar = self.tanh(np.dot(stack,self.W_c) + self.B_c)\n",
    "\n",
    "        # New Cell state:\n",
    "        cell_state = forget_gate * C_prev + input_gate * cell_bar\n",
    "\n",
    "        # Output fate:\n",
    "        output_gate = self.sigmoid(np.dot(stack,self.W_o) + self.B_o)\n",
    "\n",
    "        # Hidden state:\n",
    "        hidden_state = output_gate * self.tanh(cell_state)\n",
    "\n",
    "        # Classifiers (Softmax) :\n",
    "        dense = np.dot(hidden_state, self.W) + self.B\n",
    "        probs = self.softmax(dense)\n",
    "        \n",
    "        cache = [stack,forget_gate,input_gate,cell_bar,cell_state,output_gate,hidden_state,dense,probs]\n",
    "        return cache\n",
    "\n",
    "    def forward(self, X, h_prev, C_prev):\n",
    "        x_s,z_s,f_s,i_s = dict(), dict(), dict(), dict()\n",
    "        C_bar_s,C_s, o_s, h_s = dict(), dict(), dict(), dict()\n",
    "        v_s, y_s = dict(), dict()\n",
    "\n",
    "        h_s[-1] = h_prev\n",
    "        C_s[-1] = C_prev\n",
    "        \n",
    "        for t in range(150):\n",
    "            \n",
    "            x_s[t] = X[:,t,:]\n",
    "            cache = self.cell_forward(x_s[t], h_s[t-1], C_s[t-1])\n",
    "            z_s[t], f_s[t], i_s[t], C_bar_s[t], C_s[t], o_s[t], h_s[t],v_s[t],y_s[t] = cache\n",
    "        \n",
    "        result_cache = [z_s, f_s, i_s, C_bar_s, C_s, o_s, h_s, v_s, y_s]\n",
    "        return result_cache\n",
    "    \n",
    "    def BPTT(self, cache, Y):\n",
    "        \n",
    "        z_s, f_s, i_s, C_bar_s, C_s, o_s, h_s,v_s, y_s = cache\n",
    "        \n",
    "        dW_f = np.zeros((np.shape(self.W_f)))\n",
    "        dW_i = np.zeros((np.shape(self.W_i)))\n",
    "        dW_c = np.zeros((np.shape(self.W_c)))\n",
    "        dW_o = np.zeros((np.shape(self.W_o)))\n",
    "        dW = np.zeros((np.shape(self.W)))\n",
    "        \n",
    "        \n",
    "        dB_f = np.zeros((np.shape(self.B_f)))\n",
    "        dB_i = np.zeros((np.shape(self.B_i)))\n",
    "        dB_c = np.zeros((np.shape(self.B_c)))\n",
    "        dB_o = np.zeros((np.shape(self.B_o)))\n",
    "        dB = np.zeros((np.shape(self.B)))\n",
    "        \n",
    "        dh_next = np.zeros(np.shape(h_s[0]))\n",
    "        dC_next = np.zeros(np.shape(C_s[0]))\n",
    "        \n",
    "        # w.r.t. softmax input\n",
    "        ddense = y_s[self.last_t]\n",
    "        ddense[np.arange(len(Y)),np.argmax(Y,1)] -= 1\n",
    "\n",
    "        # Softmax classifier's :\n",
    "        \n",
    "        dW = np.dot(h_s[149].T,ddense)\n",
    "        dB = np.sum(ddense,axis = 0, keepdims = True)\n",
    "        # Backprop through time:\n",
    "        \n",
    "        for t in reversed(range(1,150)):\n",
    "            \n",
    "            C_prev = C_s[t-1]\n",
    "\n",
    "            # Output gate :\n",
    "            dh = np.dot(ddense,self.W.T) + dh_next\n",
    "            do = dh * self.tanh(C_s[t])\n",
    "            do = do * self.der_sigmoid(o_s[t])\n",
    "            dW_o += np.dot(z_s[t].T,do)\n",
    "            dB_o += np.sum(do,axis = 0, keepdims = True)\n",
    "            \n",
    "            # Cell state:\n",
    "            dC = dC_next\n",
    "            dC += dh * o_s[t] * self.der_tanh(C_s[t])\n",
    "            dC_bar = dC * i_s[t]\n",
    "            dC_bar = dC_bar * self.der_tanh(C_bar_s[t])\n",
    "            dW_c += np.dot(z_s[t].T,dC_bar)\n",
    "            dB_c += np.sum(dC_bar,axis = 0, keepdims = True)\n",
    "            \n",
    "            # Input gate:\n",
    "            di = dC * C_bar_s[t]\n",
    "            di = self.der_sigmoid(i_s[t]) * di\n",
    "            dW_i += np.dot(z_s[t].T,di)\n",
    "            dB_i += np.sum(di,axis = 0,keepdims = True)\n",
    "            \n",
    "            # Forget gate:\n",
    "            df = dC * C_prev\n",
    "            df = df * self.der_sigmoid(f_s[t])\n",
    "            dW_f += np.dot(z_s[t].T,df)\n",
    "            dB_f += np.sum(df,axis = 0, keepdims = True)\n",
    "            dz = np.dot(df,self.W_f.T) + np.dot(di,self.W_i.T) + np.dot(dC_bar,self.W_c.T) + np.dot(do,self.W_o.T)\n",
    "            dh_next = dz[:,-self.hidden_dim:]\n",
    "            dC_next = f_s[t] * dC\n",
    "            \n",
    "        # List of gradients :\n",
    "        grads = [dW,dB,dW_o,dB_o,dW_c,dB_c,dW_i,dB_i,dW_f,dB_f]\n",
    "        \n",
    "        # Clipping gradients anyway\n",
    "        for grad in grads:\n",
    "            np.clip(grad, -15, 15, out = grad)\n",
    "            \n",
    "        return h_s[self.last_t], C_s[self.last_t], grads\n",
    "    \n",
    "    def train_network(self, data, labels, test_data, test_labels, epochs = 50):\n",
    "\n",
    "        valid_loss = list()\n",
    "        valid_accuracy = list()\n",
    "\n",
    "        test_loss = list()\n",
    "        test_accuracy = list()\n",
    "\n",
    "        sample_size = np.shape(data)[0]             \n",
    "        k = int(sample_size / 10)   \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            print('Epoch : ' +str(epoch))\n",
    "            \n",
    "            randomIndexes = np.random.permutation(sample_size)\n",
    "            data = data[randomIndexes]\n",
    "            \n",
    "            h_prev,C_prev = np.zeros((self.batch_size,self.hidden_dim)),np.zeros((self.batch_size,self.hidden_dim))\n",
    "            \n",
    "            start_time = time.time()\n",
    "            number_of_batches = int(sample_size / self.batch_size)\n",
    "            for i in range(number_of_batches):\n",
    "                \n",
    "                start = int(self.batch_size*i)\n",
    "                end = int(self.batch_size*(i+1))\n",
    "                \n",
    "                # Feeding random indexes:\n",
    "                data_feed = data[start:end]\n",
    "                labels_feed = labels[start:end]\n",
    "                \n",
    "                # Forward + BPTT + SGD:\n",
    "                cache_train = self.forward(data_feed, h_prev, C_prev)\n",
    "                h,c,grads = self.BPTT(cache_train, labels_feed)\n",
    "\n",
    "                self.update_weights(grads)\n",
    "\n",
    "                # Hidden state -------> Previous hidden state\n",
    "                # Cell state ---------> Previous cell state\n",
    "                h_prev,C_prev = h,c\n",
    "                \n",
    "            end_time = time.time()\n",
    "            print('Training time for 1 epoch : ' +str(end_time - start_time))\n",
    "             \n",
    "            valid_data = data[0:k]\n",
    "            valid_labels = labels[0:k]    \n",
    "                \n",
    "            # Validation metrics calculations:\n",
    "            \n",
    "            valid_prevs = np.zeros((valid_data.shape[0],self.hidden_dim))\n",
    "            \n",
    "            valid_cache = self.forward(valid_data,valid_prevs,valid_prevs)\n",
    "            probs_valid = valid_cache[-1]            \n",
    "            \n",
    "            cross_loss_valid = self.CategoricalCrossEntropy(valid_labels, probs_valid[self.last_t])\n",
    "\n",
    "            \n",
    "            # Test metrics calculations:\n",
    "            test_prevs = np.zeros((test_data.shape[0],self.hidden_dim))\n",
    "            \n",
    "            test_cache = self.forward(test_data,test_prevs,test_prevs)\n",
    "            probs_test = test_cache[-1]\n",
    "            \n",
    "            cross_loss_test = self.CategoricalCrossEntropy(test_labels,probs_test[self.last_t])\n",
    "            predictions_test = np.argmax(probs_test[self.last_t],1)\n",
    "            acc_test = self.accuracy(np.argmax(test_labels,1),predictions_test)\n",
    "\n",
    "            valid_loss.append(cross_loss_valid)\n",
    "            test_loss.append(cross_loss_test)\n",
    "\n",
    "            test_accuracy.append(acc_test)\n",
    "            \n",
    "        return valid_loss, test_loss, test_accuracy\n",
    "              \n",
    "                  \n",
    "    def update_weights(self, grads):\n",
    "        \n",
    "        dW,dB,dW_o,dB_o,dW_c,dB_c,dW_i,dB_i,dW_f,dB_f = grads\n",
    "        \n",
    "        # If momentum is used.\n",
    "        if( self.momentum_condition == True ):\n",
    "\n",
    "            self.momentum_W_f = dW_f + (self.momentumCoef * self.momentum_W_f) \n",
    "            self.momentum_B_f = dB_f + (self.momentumCoef * self.momentum_B_f) \n",
    "            self.momentum_W_i = dW_i + (self.momentumCoef * self.momentum_W_i) \n",
    "            self.momentum_B_i = dB_i + (self.momentumCoef * self.momentum_B_i) \n",
    "            self.momentum_W_c = dW_c + (self.momentumCoef * self.momentum_W_c) \n",
    "            self.momentum_B_c = dB_c + (self.momentumCoef * self.momentum_B_c) \n",
    "            self.momentum_W_o = dW_o + (self.momentumCoef * self.momentum_W_o) \n",
    "            self.momentum_B_o = dB_o + (self.momentumCoef * self.momentum_B_o) \n",
    "            self.momentum_W = dW + (self.momentumCoef * self.momentum_W) \n",
    "            self.momentum_B = dB + (self.momentumCoef * self.momentum_B) \n",
    "            \n",
    "                \n",
    "            self.W_f -= self.learning_rate * self.momentum_W_f \n",
    "            self.B_f -= self.learning_rate * self.momentum_B_f\n",
    "            self.W_i -= self.learning_rate * self.momentum_W_i \n",
    "            self.B_i -= self.learning_rate * self.momentum_B_i \n",
    "            self.W_c -= self.learning_rate * self.momentum_W_c \n",
    "            self.B_c -= self.learning_rate * self.momentum_B_c \n",
    "            self.W_o -= self.learning_rate * self.momentum_W_o \n",
    "            self.B_o -= self.learning_rate * self.momentum_B_o \n",
    "            self.W -= self.learning_rate * self.momentum_W\n",
    "            self.B -= self.learning_rate * self.momentum_B \n",
    "            \n",
    "        # If momentum is not used.        \n",
    "        else:\n",
    "            \n",
    "            self.W_f -= self.learning_rate * dW_f\n",
    "            self.B_f -= self.learning_rate * dB_f\n",
    "            self.W_i -= self.learning_rate * dW_i\n",
    "            self.B_i -= self.learning_rate * dB_i\n",
    "            self.W_c -= self.learning_rate * dW_c\n",
    "            self.B_c -= self.learning_rate * dB_c\n",
    "            self.W_o -= self.learning_rate * dW_o\n",
    "            self.B_o -= self.learning_rate * dB_o\n",
    "            self.W -= self.learning_rate * dW\n",
    "            self.B -= self.learning_rate * dB\n",
    "                  \n",
    "                  \n",
    "    def predict(self,X):\n",
    "\n",
    "        # Give zeros to hidden/cell states:\n",
    "        pasts = np.zeros((np.shape(X)[0], self.hidden_dim))\n",
    "        \n",
    "        result_cache = self.forward(X,pasts,pasts)\n",
    "        probabilities = result_cache[-1]\n",
    "        result_prob = np.argmax(probabilities[self.last_t],axis=1)\n",
    "\n",
    "        return result_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LSTM_model = LSTM(learning_rate = 1e-15, momentumCoef = 0.85, batch_size = 32, hidden_dim=128,  momentum_condition = True)\n",
    "\n",
    "valid_loss_lstm, test_loss_lstm, test_accuracy_lstm = LSTM_model.train_network(train_data, train_labels, \n",
    "                                                                               test_data, test_labels, epochs = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figureNum += 1\n",
    "\n",
    "plt.figure(figureNum)\n",
    "plt.plot(valid_loss_lstm)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss of Validation Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_lstm = LSTM_model.predict(train_data)\n",
    "test_preds_lstm = LSTM_model.predict(test_data)\n",
    "confusion_mat_train_lstm = confusion_matrix(train_labels, train_preds_lstm)\n",
    "confusion_mat_test_lstm = confusion_matrix(test_labels, test_preds_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_LSTM_train = accuracy_(confusion_mat_train_lstm)\n",
    "print('Accuracy of LSTM with train data : ' +str(accuracy_LSTM_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_LSTM_test = accuracy_(confusion_mat_test_lstm)\n",
    "print('Accuracy of LSTM with test data : ' +str(accuracy_LSTM_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns are : PREDICTION \\n')\n",
    "print('Rows are : ACTUAL \\n')\n",
    "\n",
    "print('The confusion matrix(LSTM) for the training data : \\n \\n' +str(confusion_mat_train_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns are : PREDICTION \\n')\n",
    "print('Rows are : ACTUAL \\n')\n",
    "\n",
    "print('The confusion matrix(LSTM) for the test data : \\n \\n' +str(confusion_mat_test_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN = keras.Sequential()\n",
    "RNN.add(layers.SimpleRNN(128, batch_input_shape=[30, 150, 3]))\n",
    "RNN.add(layers.Dense(6, activation='softmax'))\n",
    "optimizer_RNN = keras.optimizers.SGD(learning_rate=0.1, momentum=0.85)\n",
    "RNN.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer_RNN,\n",
    "                      metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN = RNN.fit(train_data, train_labels, batch_size=30, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_RNN = RNN.predict_classes(train_data, batch_size = 30)\n",
    "test_preds_RNN = RNN.predict_classes(test_data, batch_size = 30)\n",
    "\n",
    "confusion_mat_train_RNN = confusion_matrix(train_labels,train_preds_RNN)\n",
    "\n",
    "confusion_mat_test_RNN = confusion_matrix(test_labels,test_preds_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_RNN_train_tf = accuracy_(confusion_mat_train_RNN)\n",
    "print('Accuracy of RNN with train data : ' +str(accuracy_RNN_train_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_RNN_test_tf = accuracy_(confusion_mat_test_RNN)\n",
    "print('Accuracy of RNN with test data : ' +str(accuracy_RNN_test_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns are : PREDICTION \\n')\n",
    "print('Rows are : ACTUAL \\n')\n",
    "\n",
    "print('The confusion matrix(RNN) for the training data : \\n \\n' +str(confusion_mat_train_RNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns are : PREDICTION \\n')\n",
    "print('Rows are : ACTUAL \\n')\n",
    "\n",
    "print('The confusion matrix(RNN) for the test data : \\n \\n' +str(confusion_mat_test_RNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM = keras.Sequential()\n",
    "LSTM.add(layers.LSTM(128, batch_input_shape=[30, 150, 3]))\n",
    "LSTM.add(layers.Dense(6, activation='softmax'))\n",
    "optimizer_LSTM = keras.optimizers.SGD(learning_rate=0.1, momentum=0.85)\n",
    "LSTM.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer_LSTM,\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = LSTM.fit(train_data, train_labels, batch_size=30, epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_LSTM = LSTM.predict_classes(train_data, batch_size = 30)\n",
    "test_preds_LSTM = LSTM.predict_classes(test_data, batch_size = 30)\n",
    "\n",
    "confusion_mat_train_LSTM = confusion_matrix(train_labels,train_preds_LSTM)\n",
    "\n",
    "confusion_mat_test_LSTM = confusion_matrix(test_labels,test_preds_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_LSTM_train_tf = accuracy_(confusion_mat_train_LSTM)\n",
    "print('Accuracy of LSTM with train data : ' +str(accuracy_LSTM_train_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_LSTM_test_tf = accuracy_(confusion_mat_test_LSTM)\n",
    "print('Accuracy of LSTM with test data : ' +str(accuracy_LSTM_test_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns are : PREDICTION \\n')\n",
    "print('Rows are : ACTUAL \\n')\n",
    "\n",
    "print('The confusion matrix(LSTM) for the training data : \\n \\n' +str(confusion_mat_train_LSTM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns are : PREDICTION \\n')\n",
    "print('Rows are : ACTUAL \\n')\n",
    "\n",
    "print('The confusion matrix(LSTM) for the test data : \\n \\n' +str(confusion_mat_test_LSTM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU = keras.Sequential()\n",
    "GRU.add(layers.GRU(128, batch_input_shape=[30, 150, 3]))\n",
    "GRU.add(layers.Dense(6, activation='softmax'))\n",
    "optimizer_GRU = keras.optimizers.SGD(learning_rate=0.1, momentum=0.85)\n",
    "GRU.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer_GRU,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "model_GRU = GRU.fit(train_data, train_labels, batch_size=30, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_GRU = GRU.predict_classes(train_data, batch_size=30)\n",
    "test_preds_GRU = GRU.predict_classes(test_data, batch_size=30)\n",
    "\n",
    "confusion_mat_train_GRU = confusion_matrix(train_labels,train_preds_GRU)\n",
    "\n",
    "confusion_mat_test_GRU = confusion_matrix(test_labels,test_preds_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_GRU_train_tf = accuracy_(confusion_mat_train_GRU)\n",
    "print('Accuracy of GRU with train data : ' +str(accuracy_GRU_train_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_GRU_test_tf = accuracy_(confusion_mat_test_GRU)\n",
    "print('Accuracy of GRU with test data : ' +str(accuracy_GRU_test_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns are : PREDICTION \\n')\n",
    "print('Rows are : ACTUAL \\n')\n",
    "\n",
    "print('The confusion matrix(GRU) for the training data : \\n \\n' +str(confusion_mat_train_GRU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns are : PREDICTION \\n')\n",
    "print('Rows are : ACTUAL \\n')\n",
    "\n",
    "print('The confusion matrix(GRU) for the test : \\n \\n' +str(confusion_mat_test_GRU))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
